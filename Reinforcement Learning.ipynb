{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abf7b6c-2c38-4a59-a56f-da93f8350463",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: red;\">\n",
    "        <h3>Reinforcement Learning:MONTE CARLO | SARSA | Q-LEARNING</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28867402-e228-41ac-9544-72478c6093c0",
   "metadata": {},
   "source": [
    "# VOLCANO PROBLEM GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ff4259-24a2-493b-a507-abb9e57597e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import numpy as np\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import numpy as np\n",
    "\n",
    "class VolcanoCrossingGUI:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"Volcano Crossing Problem\")\n",
    "\n",
    "        # Slip Probability\n",
    "        self.slip_label = ttk.Label(self.master, text=\"Slip Probability:\")\n",
    "        self.slip_label.grid(row=0, column=0, padx=10, pady=5)\n",
    "        self.slip_scale = ttk.Scale(self.master, from_=0.0, to=0.3, orient=tk.HORIZONTAL, command=self.update_slip_value)\n",
    "        self.slip_scale.set(0.1)\n",
    "        self.slip_scale.grid(row=0, column=1, padx=10, pady=5)\n",
    "\n",
    "        # Label to display slip probability value\n",
    "        self.slip_value_label = ttk.Label(self.master, text=f\"Slip Probability: {self.slip_scale.get()}\")\n",
    "        self.slip_value_label.grid(row=0, column=2, padx=10, pady=5)\n",
    "\n",
    "        # Epsilon\n",
    "        self.epsilon_label = ttk.Label(self.master, text=\"Epsilon:\")\n",
    "        self.epsilon_label.grid(row=1, column=0, padx=10, pady=5)\n",
    "        self.epsilon_scale = ttk.Scale(self.master, from_=0.0, to=1.0, orient=tk.HORIZONTAL, command=self.update_epsilon_value)\n",
    "        self.epsilon_scale.set(0.2)\n",
    "        self.epsilon_scale.grid(row=1, column=1, padx=10, pady=5)\n",
    "\n",
    "        self.results_label = ttk.Label(self.master, text=\"\")\n",
    "        self.results_label.grid(row=5, column=0, columnspan=3, padx=10, pady=5)\n",
    "        \n",
    "        \n",
    "        # Label to display epsilon value\n",
    "        self.epsilon_value_label = ttk.Label(self.master, text=f\"Epsilon: {self.epsilon_scale.get()}\")\n",
    "        self.epsilon_value_label.grid(row=1, column=2, padx=10, pady=5)\n",
    "\n",
    "        # Number of Episodes\n",
    "        self.episodes_label = ttk.Label(self.master, text=\"Number of Episodes:\")\n",
    "        self.episodes_label.grid(row=2, column=0, padx=10, pady=5)\n",
    "        self.episodes_entry = ttk.Entry(self.master)\n",
    "        self.episodes_entry.insert(0, \"1000\")\n",
    "        self.episodes_entry.grid(row=2, column=1, padx=10, pady=5)\n",
    "\n",
    "        # Run Algorithms Buttons\n",
    "        sarsa_button = ttk.Button(self.master, text=\"Run SARSA\", command=lambda: self.run_algorithm(\"SARSA\"))\n",
    "        sarsa_button.grid(row=3, column=0, padx=10, pady=5)\n",
    "\n",
    "        q_learning_button = ttk.Button(self.master, text=\"Run Q-Learning\", command=lambda: self.run_algorithm(\"Q-Learning\"))\n",
    "        q_learning_button.grid(row=3, column=1, padx=10, pady=5)\n",
    "\n",
    "        monte_carlo_button = ttk.Button(self.master, text=\"Run Monte Carlo\", command=lambda: self.run_algorithm(\"Monte Carlo\"))\n",
    "        monte_carlo_button.grid(row=3, column=2, padx=10, pady=5)\n",
    "\n",
    "        # Run All Button\n",
    "        run_all_button = ttk.Button(self.master, text=\"Run All Models\", command=self.run_all_models)\n",
    "        run_all_button.grid(row=4, column=1, padx=10, pady=5)\n",
    "\n",
    "        # Reset Button\n",
    "        reset_button = ttk.Button(self.master, text=\"Reset\", command=self.reset_gui)\n",
    "        reset_button.grid(row=4, column=0, padx=10, pady=5)\n",
    "\n",
    "\n",
    "        # Display Volcano Grid and Results\n",
    "        self.volcano_grid_canvas = tk.Canvas(self.master, width=500, height=300)\n",
    "        self.volcano_grid_canvas.grid(row=0, column=3, rowspan=5, padx=10, pady=5)\n",
    "\n",
    "        # Create environment\n",
    "        self.env = VolcanoCrossingEnvironment()\n",
    "\n",
    "        # Initialize the volcano grid\n",
    "        self.display_volcano_grid()\n",
    "\n",
    "    def update_slip_value(self, value):\n",
    "        self.slip_value_label.config(text=f\"Slip Probability: {float(value):.2f}\")\n",
    "\n",
    "    def update_epsilon_value(self, value):\n",
    "        self.epsilon_value_label.config(text=f\"Epsilon: {float(value):.2f}\")\n",
    "        \n",
    "    def display_all_results(self, results_sarsa, results_q_learning, results_monte_carlo):\n",
    "        \n",
    "        # Display results for all three models\n",
    "        results_text = f\"Results (SARSA): {results_sarsa}\\nResults (Q-Learning): {results_q_learning}\\nResults (Monte Carlo): {results_monte_carlo}\"\n",
    "        self.results_label.config(text=results_text)\n",
    "\n",
    "    def run_algorithm(self, algorithm):\n",
    "        slip_probability = self.slip_scale.get()\n",
    "        epsilon_value = self.epsilon_scale.get()\n",
    "        num_episodes = int(self.episodes_entry.get())\n",
    "    \n",
    "        # Update labels with current values\n",
    "        self.slip_value_label.config(text=f\"Slip Probability: {slip_probability}\")\n",
    "        self.epsilon_value_label.config(text=f\"Epsilon: {epsilon_value}, Number of Episodes: {num_episodes}\")\n",
    "    \n",
    "        # Create environment\n",
    "        env = VolcanoCrossingEnvironment()\n",
    "    \n",
    "        if algorithm == \"SARSA\":\n",
    "            utility, q_values = sarsa(env, num_episodes, slip_probability, epsilon_value, 0.9, 0.1)\n",
    "            self.display_results(utility, q_values, algorithm)\n",
    "            self.display_volcano_grid(q_values)  # Add this line\n",
    "        elif algorithm == \"Q-Learning\":\n",
    "            utility, q_values = q_learning(env, num_episodes, slip_probability, epsilon_value, 0.9)\n",
    "            self.display_results(utility, q_values, algorithm)\n",
    "            self.display_volcano_grid(q_values)  # Add this line\n",
    "        elif algorithm == \"Monte Carlo\":\n",
    "            utility, q_values = model_free_monte_carlo(env, num_episodes)\n",
    "            self.display_results(utility, q_values, algorithm)\n",
    "            self.display_volcano_grid(q_values)  # Add this line\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def display_results(self, utility=None, q_values=None, algorithm=None):\n",
    "        results_label_text = f\"Results ({algorithm}):\"\n",
    "    \n",
    "        if utility is not None:\n",
    "            results_label_text += f\"\\nUtility: {utility}\"\n",
    "    \n",
    "        # if q_values is not None:\n",
    "        #     results_label_text += \"\\nQ-Values:\\n\"\n",
    "        #     for row in range(q_values.shape[0]):\n",
    "        #         for col in range(q_values.shape[1]):\n",
    "        #             results_label_text += f\"({row}, {col}): {q_values[row, col]}\\n\"\n",
    "    \n",
    "        self.results_label.config(text=results_label_text)\n",
    "    \n",
    "\n",
    "    def run_all_models(self):\n",
    "        slip_probability = self.slip_scale.get()\n",
    "        epsilon_value = self.epsilon_scale.get()\n",
    "        num_episodes = int(self.episodes_entry.get())\n",
    "\n",
    "        # Create environment\n",
    "        env = VolcanoCrossingEnvironment()\n",
    "\n",
    "        # Run all three algorithms\n",
    "        results_sarsa,Q = sarsa(env, num_episodes, slip_probability, epsilon_value, 0.9, 0.1)\n",
    "        results_q_learning,Q = q_learning(env, num_episodes, slip_probability, epsilon_value, 0.9)\n",
    "        results_monte_carlo,Q = model_free_monte_carlo(env, num_episodes)\n",
    "\n",
    "        self.display_all_results(results_sarsa, results_q_learning, results_monte_carlo)\n",
    "\n",
    "    def display_volcano_grid(self, q_values=None):\n",
    "        # Display logic for the volcano grid\n",
    "        self.volcano_grid_canvas.delete(\"all\")  # Clear the canvas\n",
    "    \n",
    "        cell_width = 100\n",
    "        cell_height = 100\n",
    "    \n",
    "        if q_values is not None:\n",
    "            for i in range(q_values.shape[0]):\n",
    "                for j in range(q_values.shape[1]):\n",
    "                    x1 = j * cell_width\n",
    "                    y1 = i * cell_height\n",
    "                    x2 = x1 + cell_width\n",
    "                    y2 = y1 + cell_height\n",
    "    \n",
    "                    fill_color = \"white\"\n",
    "                    outline_color = \"black\"\n",
    "    \n",
    "                    # Display the Q-values in four quadrants of the cell\n",
    "                    q_text = [f\"{q_values[i, j][0]:.2f}\", f\"{q_values[i, j][1]:.2f}\", f\"{q_values[i, j][2]:.2f}\", f\"{q_values[i, j][3]:.2f}\"]\n",
    "    \n",
    "                    label_text = q_text\n",
    "    \n",
    "                    # Determine the fill color based on the state\n",
    "                    if (i, j) in self.env.volcano_states:\n",
    "                        fill_color = \"red\"\n",
    "                    elif (i, j) in self.env.reward_states:\n",
    "                        fill_color = \"blue\"\n",
    "    \n",
    "                    self.volcano_grid_canvas.create_rectangle(x1, y1, x2, y2, fill=fill_color, outline=outline_color)\n",
    "                    # Display Q-values in each quadrant\n",
    "                    self.volcano_grid_canvas.create_text((x1 + x2) // 2, y1 + 15, text=label_text[0], justify=tk.CENTER)  # Top-center\n",
    "                    self.volcano_grid_canvas.create_text(x1 + 7, (y1 + y2) // 2, text=label_text[1], anchor='nw', justify=tk.LEFT)  # Left\n",
    "                    self.volcano_grid_canvas.create_text(x2 - 7, (y1 + y2) // 2, text=label_text[2], anchor='ne', justify=tk.RIGHT)  # Right\n",
    "                    self.volcano_grid_canvas.create_text((x1 + x2) // 2, y2 - 15, text=label_text[3], justify=tk.CENTER)  # Bottom-cente\n",
    "                    # Draw diagonals\n",
    "                    self.volcano_grid_canvas.create_line(x1, y1, x2, y2)  # Top-left to bottom-right\n",
    "                    self.volcano_grid_canvas.create_line(x2, y1, x1, y2)  # Top-right to bottom-left\n",
    "        else:\n",
    "            for i in range(self.env.grid_size[0]):\n",
    "                for j in range(self.env.grid_size[1]):\n",
    "                    x1 = j * cell_width\n",
    "                    y1 = i * cell_height\n",
    "                    x2 = x1 + cell_width\n",
    "                    y2 = y1 + cell_height\n",
    "    \n",
    "                    fill_color = \"white\"\n",
    "                    outline_color = \"black\"\n",
    "    \n",
    "                    if (i, j) in self.env.volcano_states:\n",
    "                        label_text = f\"Volcano:{self.env.volcano_penalty}\"\n",
    "                        fill_color = \"red\"\n",
    "                    elif (i, j) in self.env.reward_states:\n",
    "                        reward_index = self.env.reward_states.index((i, j))\n",
    "                        label_text = f\"Reward: {self.env.reward_values[reward_index]}\"\n",
    "                        fill_color = \"green\"\n",
    "                    elif (i, j) == self.env.start_state:\n",
    "                        label_text = \"Start\"\n",
    "                        fill_color = \"blue\"\n",
    "                    else:\n",
    "                        label_text = \"Empty\"\n",
    "    \n",
    "                    self.volcano_grid_canvas.create_rectangle(x1, y1, x2, y2, fill=fill_color, outline=outline_color)\n",
    "                    self.volcano_grid_canvas.create_text((x1 + x2) // 2, (y1 + y2) // 2, text=label_text, justify=tk.CENTER)\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def reset_gui(self):\n",
    "        # Reset all values to their initial state\n",
    "        self.slip_scale.set(0.1)\n",
    "        self.epsilon_scale.set(0.2)\n",
    "        self.episodes_entry.delete(0, tk.END)\n",
    "        self.episodes_entry.insert(0, \"1000\")\n",
    "        self.slip_value_label.config(text=f\"Slip Probability: {self.slip_scale.get()}\")\n",
    "        self.epsilon_value_label.config(text=f\"Epsilon: {self.epsilon_scale.get()}\")\n",
    "        self.display_volcano_grid()  # Reset the volcano grid\n",
    "        self.results_label.config(text=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be47e4-ffda-4b47-b210-d0691ac1860f",
   "metadata": {},
   "source": [
    "# VOLCANO PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910cf0d-bb9e-4c83-a5d7-b443db5828e7",
   "metadata": {},
   "source": [
    "<p><b>The Volcano Crossing environment is a 3x4 grid world where an agent starts at position (0, 0). The goal is to navigate to rewarding states (2, 0) and (0, 3) while avoiding volcanic states (0, 2) and (1, 2). Movement actions include \"up,\" \"down,\" \"left,\" and \"right,\" each leading to a new state. Successful moves yield a reward of 1, reaching volcanic states incurs a penalty of -50, and reaching rewarding states results in rewards of 2 and 20, respectively. The environment operates under a discounted factor (gamma) of 0.9 and a learning rate (alpha) of 0.1.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e470dfbd-f798-4cb1-9709-25def00ee703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolcanoCrossingEnvironment:\n",
    "    def __init__(self, gamma=0.9, alpha=0.1):\n",
    "        self.grid_size = (3, 4)\n",
    "        self.start_state = (0, 0)\n",
    "        # self.end_states = [(3, 1), (1, 4)]\n",
    "        self.volcano_states = [(0, 2), (1, 2)]\n",
    "        self.volcano_penalty = -50\n",
    "        self.reward_states = [(2, 0), (0, 3)]\n",
    "        self.reward_values = [2, 20]\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        # self.start_state = (0, 0)\n",
    "        return self.start_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the environment step based on the chosen action\n",
    "\n",
    "        # Define the reward for a successful move\n",
    "        success_reward = 1\n",
    "\n",
    "        # Check if the action is valid\n",
    "        if action not in self.actions:\n",
    "            # Invalid action, return the current state\n",
    "            return self.start_state, 0, False\n",
    "\n",
    "        # Map action to direction\n",
    "        direction = {\"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)}[action]\n",
    "\n",
    "        # Calculate new state based on the chosen action\n",
    "        new_state = (self.start_state[0] + direction[0], self.start_state[1] + direction[1])\n",
    "\n",
    "        # Check if the new state is within the grid boundaries\n",
    "        if 0 <= new_state[0] < self.grid_size[0] and 0 <= new_state[1] < self.grid_size[1]:\n",
    "            # Check if the new state is a volcano\n",
    "            if new_state in self.volcano_states:\n",
    "                reward = self.volcano_penalty\n",
    "                done = True\n",
    "            elif new_state in self.reward_states:\n",
    "                # Get the index of the reward state to fetch the corresponding reward value\n",
    "                reward_index = self.reward_states.index(new_state)\n",
    "                reward = self.reward_values[reward_index]\n",
    "                done = True\n",
    "            else:\n",
    "                reward = success_reward\n",
    "                done = False\n",
    "        else:\n",
    "            # Out of bounds, return the current state\n",
    "            return self.start_state, 0, False\n",
    "\n",
    "        # Update the current state\n",
    "        self.start_state = new_state\n",
    "\n",
    "        return new_state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f8a426-4cdb-4e4d-881e-ed8100f8ea0b",
   "metadata": {},
   "source": [
    "# Monte Carlo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfe787-634d-4dd7-929b-aa47857b8fbf",
   "metadata": {},
   "source": [
    "<p><b>The model free Monte Carlo is implemented to learn the optimal policy by sampling episodes. it estimate the state action   values through random transition of sates in grid environment using any random policy and perform that action and get next state and reward and update q values accordingly. With increase in number of episodes the average utility improved gradually</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bed560d-70b5-4011-abad-b18012a4896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def model_free_monte_carlo(env, num_episodes, max_steps=1000):\n",
    "    num_actions = len(env.actions)\n",
    "    Q = np.zeros((env.grid_size[0], env.grid_size[1], num_actions))\n",
    "    returns_sum = np.zeros((env.grid_size[0], env.grid_size[1], num_actions))\n",
    "    returns_count = np.zeros((env.grid_size[0], env.grid_size[1], num_actions))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, done = env.reset(), False\n",
    "        episode_data = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Epsilon-greedy action selection\n",
    "            epsilon = 0.1  # You can adjust this value\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(num_actions)\n",
    "            else:\n",
    "                action = np.argmax(Q[state[0], state[1]])\n",
    "\n",
    "            next_state, reward, done = env.step(env.actions[action])\n",
    "            episode_data.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        G = 0\n",
    "        for t in range(len(episode_data) - 1, -1, -1):\n",
    "            state, action, reward = episode_data[t]\n",
    "            G = env.gamma * G + reward\n",
    "\n",
    "            if (state[0], state[1], action) not in [(x[0], x[1], x[2]) for x in episode_data[:t]]:\n",
    "                returns_sum[state[0], state[1], action] += G\n",
    "                returns_count[state[0], state[1], action] += 1\n",
    "                Q[state[0], state[1], action] = returns_sum[state[0], state[1], action] / returns_count[state[0], state[1], action]\n",
    "\n",
    "    # Calculate overall utility based on the maximum Q-value in the final state\n",
    "    overall_utility = np.max(Q[state[0], state[1]])\n",
    "    print(f\"Overall Utility: {overall_utility}\")\n",
    "    return overall_utility,Q\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11955cd-fded-4875-bdb5-030ecf9aaf39",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef39b41-20af-45da-865d-29b7c136ee35",
   "metadata": {},
   "source": [
    "<p><b>SARSA uses on policy method, and it updates Q-values based on current policy based on agents sate transition. Actions are selected using epsilon greedy policy, and then execute action and observe next state and reward. Changing number of episodes shows convergence in Q-values.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041fd8cf-dc66-4420-b8cb-27415ed8a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_policy(Q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: Choose a random action\n",
    "        return np.random.randint(len(Q_values))\n",
    "    else:\n",
    "        # Exploit: Choose the action with the highest Q-value\n",
    "        return np.argmax(Q_values)\n",
    "\n",
    "def sarsa(env, num_episodes, initial_epsilon, alpha, gamma, epsilon_decay, min_epsilon=0.01, convergence_threshold=1e-6):\n",
    "    Q = np.zeros((env.grid_size[0], env.grid_size[1], len(env.actions)))\n",
    "    epsilon = initial_epsilon\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, done = env.reset(), False\n",
    "        action_index = epsilon_greedy_policy(Q[state], epsilon)\n",
    "        action = env.actions[action_index]\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action_index = epsilon_greedy_policy(Q[next_state], epsilon)\n",
    "            next_action = env.actions[next_action_index]\n",
    "            Q[state[0], state[1], action_index] += alpha * (\n",
    "                reward + gamma * Q[next_state[0], next_state[1], next_action_index] - Q[state[0], state[1], action_index]\n",
    "            )\n",
    "            state, action, action_index = next_state, next_action, next_action_index\n",
    "\n",
    "            # Decay epsilon with a minimum value\n",
    "            epsilon = max(epsilon * epsilon_decay, min_epsilon)\n",
    "\n",
    "        # Check for convergence\n",
    "        if episode > 0:\n",
    "            if np.max(np.abs(Q - prev_Q)) < convergence_threshold:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "        # Update previous Q-values for convergence check\n",
    "        prev_Q = np.copy(Q)\n",
    "\n",
    "    # Calculate overall utility based on the average of maximum Q-values over all states\n",
    "    overall_utility = np.mean(np.max(Q, axis=2))\n",
    "    print(f\"Overall Utility: {overall_utility}\")\n",
    "\n",
    "    return overall_utility,Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36655e9-e197-4e46-8f7b-23005c53ff91",
   "metadata": {},
   "source": [
    "# Q-LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e56201-5bf5-4a12-bcec-9435c5508118",
   "metadata": {},
   "source": [
    "<p><b>Q-Learning is off- policy and it updates values based on future reward and it is using epsilon greedy policy for exploration  that will select the action to be taken like selecting initial state or initial action and then execute action and observe next state and reward.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc86e24a-eb30-4da5-aa2b-53c46070b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def q_learning(env, num_episodes, epsilon, alpha, gamma):\n",
    "    Q = np.zeros((env.grid_size[0], env.grid_size[1], len(env.actions)))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, done = env.reset(), False\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(Q[state], epsilon)\n",
    "            next_state, reward, done = env.step(env.actions[action])\n",
    "\n",
    "            # Q-value update\n",
    "            Q[state[0], state[1], action] += alpha * (\n",
    "                reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action]\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    # Calculate overall utility based on the average of maximum Q-values over all states\n",
    "    overall_utility = np.mean(np.max(Q, axis=2))\n",
    "    print(f\"Overall Utility: {overall_utility}\")\n",
    "\n",
    "    return overall_utility,Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3fbc17-d6f7-4301-a67c-e15a17626b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2128\\1705972342.py\", line 77, in update_slip_value\n",
      "    self.slip_value_label.config(text=f\"Slip Probability: {float(value):.2f}\")\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'VolcanoCrossingGUI' object has no attribute 'slip_value_label'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python312\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_2128\\1705972342.py\", line 80, in update_epsilon_value\n",
      "    self.epsilon_value_label.config(text=f\"Epsilon: {float(value):.2f}\")\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'VolcanoCrossingGUI' object has no attribute 'epsilon_value_label'. Did you mean: 'slip_value_label'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Utility: 49.376469148464615\n",
      "Overall Utility: 5.28410935517971\n",
      "Overall Utility: 50.05547371670389\n",
      "Overall Utility: 17.407890109890108\n",
      "Overall Utility: 6.371972067330781\n"
     ]
    }
   ],
   "source": [
    "# Create GUI\n",
    "root = tk.Tk()\n",
    "app = VolcanoCrossingGUI(root)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d1657f-d804-4938-9f56-a2724e6fef67",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92251ce-1ea2-4fbc-871c-a384d8848c78",
   "metadata": {},
   "source": [
    "<p><b>All three algorithms (Model-Free Monte Carlo, SARSA, and Q-Learning) were applied to the Volcano Crossing Problem.  Model-Free Monte Carlo relies on Monte Carlo sampling and showed effectiveness in learning optimal policies. SARSA and Q-Learning, being temporal difference methods update Q-values and converge to optimal policies. The slip probability, epsilon (exploration parameter), and the number of episodes significantly impact algorithm performance. Higher epsilon values encouraged more exploration, potentially increasing Q-values. SARSA and Q-Learning algorithms included convergence checks to monitor changes in Q-values, ensuring stability. Model-Free Monte Carlo may not explicitly check for convergence due to its episodic nature but showed effectiveness in learning.</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36da3c-35f1-458b-91e3-674454e0e9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1606a4e5-eb97-422d-945b-7ad721de3e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
